{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd import numpy as np import csv\n",
    "\n",
    "#bibliothèques pour le preprocessing from sklearn.model_selection import\n",
    "train_test_split from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#https://colab.research.google.com/drive/1ogck5g9XX_eob0UPyg3Deq5zBYDIAwRm#scrollTo=SAOJH8sHO-OH\n",
    "\n",
    "tit = pd.read_csv(“C:/TP1/titanic.csv”)\n",
    "\n",
    "# Chargement du fichier titanic sous la formed’un dataframe Pandae tit\n",
    "\n",
    "#afficher le type de la variable, ici un dataframe type(tit);\n",
    "\n",
    "#afficher les 5 premieres lignes du tableau #et leur contenu\n",
    "print(tit.head());\n",
    "\n",
    "#afficher les colonnes du dataframe print(tit.columns);\n",
    "\n",
    "#1ère statistiques print(tit\\[“Survived”\\].value_counts());\n",
    "\n",
    "print() print()\n",
    "\n",
    "#QUESTIONS AUXQUELLES REPONDRE\n",
    "\n",
    "#Chercher désormais la répartition de femmes et d’hommes dans les\n",
    "données. #Combien y a t-il d’hommes dans le dataset ?\n",
    "print(tit\\[“Sex”\\].value_counts());\n",
    "\n",
    "#afficher plus spécifique les hommes print((tit\\[“Sex”\\] ==\n",
    "“male”).sum());\n",
    "\n",
    "print() print()\n",
    "\n",
    "#Cherchez la répartition des passagers parmi les différentes classes\n",
    "(1ère classe, 2ème classe, 3ème classe).\n",
    "print(tit\\[“Pclass”\\].value_counts()) #vérifier nb total de passager\n",
    "print(len(tit\\[“Pclass”\\]))\n",
    "\n",
    "print() print()\n",
    "\n",
    "#Méthode plus précise print(tit.groupby(\\[“Sex”, “Pclass”\\])\n",
    "\\[“Survived”\\].mean()\\* 100) #On observe d’une part que peu importe les\n",
    "classses, la feature sexe #montre que les femmes ont plus survécues que\n",
    "les hommes, en bien plus grand nombres que les hommes print()\n",
    "\n",
    "print(‘\\033\\[1mNous pouvons déjà observer certaines caractéristiques :\n",
    "comment les features Sex et pClass influencent elles les chances de\n",
    "survie des passagers ?\\033\\[0m’)\n",
    "\n",
    "““” D’après les résultats par la moyenne de survie selon le groupement\n",
    "par sexe et classe, nous pouvons observer d’une part que peu importe les\n",
    "classses, la feature sexe montre que les femmes ont plus survécues que\n",
    "les hommes, en bien plus grand nombres que les hommes. D’autre part, la\n",
    "feature PClass montre que ce sont les personnes membres des classes 1\n",
    "qui ont le plus eut de chances de survie chez les femmes et surtout chez\n",
    "les hommes, avec des gros écart entre la classe 1 et les 2 autres\n",
    "classes (moins marquées chez les femmes) ensuite les personnes des\n",
    "classes 2, puis 3. Ca montre une forte corrélation entre le sexe, puis\n",
    "la classe qui ont semble jouer un role pour la survie ““”\n",
    "\n",
    "print()\n",
    "\n",
    "print(‘\\033\\[1mCalculer le prix moyen (feature Fare) des tickets sur\n",
    "tous les passagers.\\033\\[0m’)\n",
    "\n",
    "res = tit\\[“Fare”\\].mean(); print(f”le prix moyen des tickets est de:\n",
    "{res: .2f} euro”)\n",
    "\n",
    "print() print()\n",
    "\n",
    "print(‘\\033\\[1mCalculer le prix moyen des tickets, en divisant les\n",
    "passagers selon leur classe.\\033\\[0m’)\n",
    "\n",
    "# Calculer le prix moyen des tickets par classe\n",
    "\n",
    "prix_moyen_classes = tit.groupby(“Pclass”)\\[“Fare”\\].mean()\n",
    "\n",
    "# Afficher le résultat\n",
    "\n",
    "print(“Prix moyen des tickets par classe (en euros) :”)\n",
    "print(prix_moyen_classes)\n",
    "\n",
    "print() print()\n",
    "\n",
    "print(‘\\033\\[1mQuel est lâge moyen des passagers sur le\n",
    "bateau?\\033\\[0m’) age_moyen = tit\\[“Age”\\].mean(); print(f”Lage moyen\n",
    "des passagers était de: {age_moyen: .2f} ans”);\n",
    "\n",
    "print() print()\n",
    "\n",
    "print(“Autre caractéristique des tableaux pandas : Sélection de\n",
    "sous-tableau”)\n",
    "\n",
    "nombre_enfants = len(tit\\[tit\\[“Age”\\] \\< 15\\]) print(f”Nombre de\n",
    "passagers de moins de 15 ans : {nombre_enfants}“) print(”Il y a 78\n",
    "passagers de moins de 15 ans.”)\n",
    "\n",
    "print() print() print() print(‘\\033\\[1m# Random Forest\\033\\[0m’)\n",
    "#algorithmes de machine learning #lalgorithme Random Forest da la\n",
    "librairie $scikit-learn$ pour prédire si un passager survivra ou non\n",
    "#Random Forest de scikit_learn ne peut qu’utiliser des features float ou\n",
    "pouvant se transformer en float (integer, ou booléen)\n",
    "\n",
    "““” On cherchera à prédire, à partir de toutes les autres\n",
    "caractéristiques, si un passagers survivra ou non. Pour cela nous allons\n",
    "créer un vecteur avec les labels (1 ou 0, pour oui ou non sur la colonne\n",
    "“Survived”) pour chacun de nos passagers.\n",
    "\n",
    "““”\n",
    "\n",
    "#En machine learning, on sépare toujours : #X : Les données utilisées\n",
    "pour faire la prédiction (features) #y : La variable que l’on veut\n",
    "prédire (label)\n",
    "\n",
    "y = tit\\[“Survived”\\] \\# on isole la variable cible qu’un veut prédire\n",
    "“Survived”\n",
    "\n",
    "#On crée ensuite un dataset contenant les features X =\n",
    "tit.drop(“Survived”, axis = 1)\n",
    "\n",
    "““” Preprocessing Certaines de nos features sont de type string comme\n",
    "“Embarked” ou “Name” par exemple. Cela pose problème : Random Forest de\n",
    "scikit_learn ne peut qu’utiliser des features float ou pouvant se\n",
    "transformer en float (integer, ou booléen).\n",
    "\n",
    "Pour pouvoir utiliser le Random Forest de scikit_learn, nous allons\n",
    "d’abord nous débarasser des colonnes difficiles (pour faciliter le\n",
    "problème) telles que “Name” ou “Ticket”.\n",
    "\n",
    "““”\n",
    "\n",
    "X = X.drop(\\[“Name”,“Ticket”\\], axis = 1)\n",
    "\n",
    "““” Désormais, nous allons renommer les valeurs de la feature Cabin pour\n",
    "sélectionner uniquement la lettre du pont correspondant : le numéro de\n",
    "cabine “C128” devient uniquement “C”. ““”\n",
    "\n",
    "# Extrait la première lettre de chaque valeur dans la colonne “Cabin”\n",
    "\n",
    "#Par exemple, si une cabine est notée “C123”, cette ligne la transforme\n",
    "en “C”\n",
    "\n",
    "X\\[“Cabin”\\] = X\\[“Cabin”\\].fillna(‘N’) \\# Remplace les valeurs\n",
    "manquantes (NaN) par N X\\[“Cabin”\\] = X\\[“Cabin”\\].apply(lambda x :\n",
    "x\\[0\\]) X\\[“Cabin”\\].value_counts().head() #on observe que bcp de\n",
    "cabines sont en N donc ne sont pas renseignées N\n",
    "\n",
    "““” Nous allons transformer les features de type string restantes en\n",
    "plusieurs colonnes contenant des booléens (car les booléens peuvent\n",
    "facilement se transformer en float : True donne 1 et False donne 0).\n",
    "\n",
    "Commençons par la feature “Sex” et transformons là en deux colonnes :\n",
    "une “Sex_female” et une “Sex_male”. Désormais, un passager aura deux\n",
    "features “female” et “male”, avec uniquement une des deux features qui\n",
    "sera égale à True, et l’autre à False.\n",
    "\n",
    "““”\n",
    "\n",
    "#La fonction transforme la colonne en deux colonnes binaires X_encoded =\n",
    "pd.get_dummies(X, columns=\\[‘Sex’\\]) #X_encoded est un nouveau DataFrame\n",
    "où la colonne “Sex” a été remplacée par Sex_female et Sex_male\n",
    "\n",
    "#afficher les 1eres colonnes pr vérifier si les modifications de\n",
    "colonnes ont bien été faites: print(X_encoded.head());\n",
    "\n",
    "print() print(‘\\033\\[1m Utilisez la méthode des dummies pour transformer\n",
    "la feature “Embarked” de type string en plusieurs colonnes de type\n",
    "booléen. Attention ! Pensez bien à modifier le dataset avec nos\n",
    "modifications.\\033\\[0m’) \\# Appliquer pd.get_dummies() sur la colonne\n",
    "“Embarked” #Cette commande transforme la colonne “Embarked” en trois\n",
    "colonnes en booléen X_encoded = pd.get_dummies(X_encoded,\n",
    "columns=\\[‘Embarked’\\])\n",
    "\n",
    "#La colonne “Embarked” (qui contient “S”, “C”, “Q”) est supprimée et 3\n",
    "nvll colonnnes en booléen sont crées print(X_encoded.head(100))\n",
    "\n",
    "print() print(‘\\033\\[1m Faaites de même pour la feature\n",
    "“Cabin”.\\033\\[0m’) X_encoded = pd.get_dummies(X_encoded,\n",
    "columns=\\[‘Cabin’\\]) print(X_encoded.columns) \\# voir la liste des\n",
    "colonnes pour vérifier\n",
    "\n",
    "#pd.set_option(‘display.max_columns’, None) \\# sinon Affiche toutes les\n",
    "colonnes #print(X_encoded.head())\n",
    "\n",
    "““” On divise désormais nos données en deux : les données\n",
    "d’entraînement, utilisées pour l’apprentissage du modèle, et les données\n",
    "de test, utilisées pour tester le modèle. \\\\\n",
    "\n",
    "Ici nous avons $test\\_size = 0.2$ donc les données de test\n",
    "représenteront $20\\%$ des données totales, et les données d’entraînement\n",
    "$80\\%$. \\\\\n",
    "\n",
    "La valeur de random_state correspond au choix d’une clé qui génère des\n",
    "nombres aléatoirement. ““”\n",
    "\n",
    "1.  Division des données X_train, X_test, y_train, y_test =\n",
    "    train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "    #random_state=42, mettre une “graîne aléatoire” pour que la\n",
    "    séparation soit reproductible (toujours les mêmes données dans train\n",
    "    et test) #Objectif : Séparer les données en deux parties : #80% pour\n",
    "    l’entraînement (X_train, y_train) : Le modèle apprend sur ces\n",
    "    données #20% pour le test (X_test, y_test) : On utilise ces données\n",
    "    pour évaluer la performance du modèle #Pourquoi ? :Si on utilise les\n",
    "    mêmes données pour entraîner et tester le modèle, on ne peut pas\n",
    "    savoir s’il généralise bien à de nouvelles données\n",
    "\n",
    "#Initialisons désormais un Random Forest avec 100 arbres. rf_model =\n",
    "RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#entrainer le random forest, du modèle rf_model.fit(X_train, y_train)\n",
    "\n",
    "#calculer la prédiction de l’algo à partir du dataset #Utiliser le\n",
    "modèle entraîné pour prédire la survie (y_pred) sur les données de test\n",
    "(X_test) y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#On calcule désormais l’erreur de prédiction du Random Forest : erreur =\n",
    "np.mean(np.abs(y_pred-y_test)) print(f”L’erreur sur le test est de\n",
    "{erreur \\* 100:.2f}%.”)\n",
    "\n",
    "accuracy = 1 - erreur print(f”L’accuracy sur le test est de {accuracy \\*\n",
    "100:.2f}%.”)\n",
    "\n",
    "print() print(‘\\033\\[1m Runnez une nouvelle fois les cases avec un\n",
    "nombre darbres de 1 et de 10. Notez les résultats.Quelle est linfluence\n",
    "du nombre darbre sur la performance du Random Forest ?\\033\\[0m’)\n",
    "\n",
    "rf_model_1 = RandomForestClassifier(n_estimators=1, random_state=42)\n",
    "rf_model_1.fit(X_train, y_train) y_pred_1 = rf_model_1.predict(X_test)\n",
    "erreur_1 = np.mean(np.abs(y_pred_1 - y_test)) accuracy_1 = 1 - erreur_1\n",
    "print(f”Nombre d’arbres = 1 : Erreur = {erreur_1 \\* 100:.2f}%, Accuracy\n",
    "= {accuracy_1 \\* 100:.2f}%“)\n",
    "\n",
    "rf_model_10 = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf_model_10.fit(X_train, y_train) y_pred_10 =\n",
    "rf_model_10.predict(X_test) erreur_10 = np.mean(np.abs(y_pred_10 -\n",
    "y_test)) accuracy_10 = 1 - erreur_10 print(f”Nombre d’arbres = 10 :\n",
    "Erreur = {erreur_10 \\* 100:.2f}%, Accuracy = {accuracy_10 \\* 100:.2f}%“)\n",
    "\n",
    "““” On peut donc observer que le nombre d’arbres a bien une influence\n",
    "sur l’analyse du jeux de données Avec 1 arbre, le modèle est imprécis\n",
    "(taux d’erreur de 33%), et fait du sous-apprentissage ou underfitting.\n",
    "En effet, le nombre d’erreur est donc plus élevé car cela ne lui permet\n",
    "pas de faire un pattern fiable et de généraliser sur les données. C’est\n",
    "pourquoi à partir de 10 arbres le modèle commence à mieux généraliser,\n",
    "le taux d’erreur diminue donc (d’environ 12%) car le modèle devient plus\n",
    "fiable. Enfin, un modèle à 100 arbres est plus précis, avec un taux\n",
    "d’erreur plus faible (seulement 17 %), car il peut généraliser sur\n",
    "plusieurs cas avec le nombre d’abres adapté à la taille du dataset.\n",
    "\n",
    "““” print() print() print() print(“Affichage dun Decision Tree”)\n",
    "\n",
    "““” Nous allons désormais entraîner un decision tree (et donc un seul\n",
    "arbre!), pour pouvoir visualiser l’arbre résultant.\n",
    "\n",
    "Affichons désormais l’arbre de décision correspondant (il n’est pas du\n",
    "tout nécessaire de lire ni de comprendre la fonction\n",
    "print_tree_horizontal, il suffit juste de l’exécuter et de regarder sa\n",
    "sortie).\n",
    "\n",
    "““”\n",
    "\n",
    "#Un DT est modèle de machine learning qui prédit en posant des qs, ici\n",
    "si une variable cible survie #Chaque nœud de l’arbre représente une\n",
    "question sur une feature (“Sex_male ≤ 0.5 ?”) #Chaque feuille donne la\n",
    "prédiction finale (ex: “Class=True” signifie que le passager a survécu)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(max_depth = 3, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "def print_tree_horizontal(tree, feature_names, node_id=0, indent=““):”“”\n",
    "Fonction récursive pour afficher l’arbre de décision de manière\n",
    "horizontale.\n",
    "\n",
    "    Parameters:\n",
    "    - tree : l'arbre à parcourir\n",
    "    - feature_names : noms des caractéristiques utilisées pour les splits\n",
    "    - node_id : identifiant du nœud actuel\n",
    "    - indent : string qui permet de formater l'affichage des niveaux de l'arbre\n",
    "    \"\"\"\n",
    "    # Si c'est une feuille\n",
    "    if tree.children_left[node_id] == -1 and tree.children_right[node_id] == -1:\n",
    "        values = tree.value[node_id]\n",
    "        classification = 'True' if values[0][1] > values[0][0] else 'False'\n",
    "        n_samples = tree.n_node_samples[node_id]\n",
    "        print(f\"{indent}Leaf: Class={classification}, Samples={n_samples}\")\n",
    "    else:\n",
    "        # Si c'est un nœud de décision\n",
    "        feature = feature_names[tree.feature[node_id]]\n",
    "        threshold = tree.threshold[node_id]\n",
    "\n",
    "        # Affiche la condition pour ce nœud\n",
    "        print(f\"{indent}If {feature} <= {threshold:.2f}?\")\n",
    "\n",
    "        # Branche True (gauche)\n",
    "        print(f\"{indent}--> True:\")\n",
    "        print_tree_horizontal(tree, feature_names, tree.children_left[node_id], indent + \"    \")\n",
    "\n",
    "        # Branche False (droite)\n",
    "        print(f\"{indent}--> False:\")\n",
    "        print_tree_horizontal(tree, feature_names, tree.children_right[node_id], indent + \"    \")\n",
    "\n",
    "print_tree_horizontal(decision_tree.tree\\_, X_encoded.columns)\n",
    "\n",
    "print(‘\\033\\[1m Quel est la feature la plus importante pour déterminer\n",
    "la survie ou non dun passager ?\\033\\[0m’)\n",
    "\n",
    "““” Dans l’exemple ci-dessus, la première question est Sex_male \\<= 0.5,\n",
    "ce qui signifie que le sexe est la feature la plus déterminante pour\n",
    "prédire la survie. L’arbre de décision montre que le modèle utilise\n",
    "d’abord le sexe pour séparer les survivants des non-survivants. Le\n",
    "modèle a compris que le sexe est le meilleur critère pour séparer les\n",
    "survivants des non-survivants ce qui fait écho à nos premières analyses\n",
    "sur le jeux de données dont les résultats ont montré que le sexe est\n",
    "fortement corrélé à la survie, bien plus que la feature classe.\n",
    "\n",
    "Après le sexe, l’arbre utilise l’age. Il montre que les enfants ont plus\n",
    "de chances de survivre, surtout ceux ≤ 6.5 ans. Sur le Titanic, on\n",
    "suppose et déduit donc que les femmes et enfants ont été évacués en\n",
    "priorité. Enfin, les autres features importantes sont Pclass, on observe\n",
    "que les passagers de 1ère classe ont plus de chances de survivre et que\n",
    "dans cette logique, la la feature Fare montre que les passagers ayant\n",
    "payé un tarif élevé ont plus de chances de survivre. Par exemple, Les\n",
    "hommes en première classe (Pclass = 1) ou ayant payé un tarif élevé\n",
    "(Fare \\> 23.35) ont plus de chances de survivre.\n",
    "\n",
    "En conclusion, les autres features comme Pclass, Age, et Fare, CAbin etc\n",
    "sont importantes, mais bien moins que le sexe qui est le facteur\n",
    "déterminant pour prédire la survie des passagers du Titanic.\n",
    "\n",
    "““”"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
